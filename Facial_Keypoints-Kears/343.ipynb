{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "343.ipynb",
      "provenance": [],
      "mount_file_id": "1cxo8Vc_l8xum36tIO0XWhZg7vP7i6B8S",
      "authorship_tag": "ABX9TyMFRe072D/2W8RDOPKM8BvL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Omarosman924/Code-force/blob/main/Facial_Keypoints-Kears/343.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEnqcm2XHL_I"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import load_img,img_to_array,array_to_img "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHwbHEykZi8W"
      },
      "source": [
        "Load Data from Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD9dd6S-HU-H"
      },
      "source": [
        "key_pts_frame = pd.read_csv('/content/drive/MyDrive/P1_Facial_Keypoints-master/data/training_frames_keypoints.csv')  \n",
        "test_data = pd.read_csv('/content/drive/MyDrive/P1_Facial_Keypoints-master/data/test_frames_keypoints.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxknAN0qIR44"
      },
      "source": [
        "train_imgs_name = key_pts_frame.iloc[:,0].values\n",
        "train_key_pts = key_pts_frame.iloc[:,1:].values.reshape(3462,68,2)\n",
        "test_imgs_name = test_data.iloc[:,0].values\n",
        "test_key_pts = test_data.iloc[:,1:].values.reshape(770,68,2)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZoIgEi7Zoev"
      },
      "source": [
        "to Display the images and key points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sN7AX6qPO05y"
      },
      "source": [
        "def show_keypoints(image, key_pts):\n",
        "    \"\"\"Show image with keypoints\"\"\"\n",
        "    plt.imshow(image)\n",
        "    plt.scatter(key_pts[:, 0], key_pts[:, 1], s=20, marker='.', c='m')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1wEMR26aDrX"
      },
      "source": [
        "Convet images to array and Rescale the keypoints to be suitable with new image Size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puwUd65sIX6u"
      },
      "source": [
        "def load_data(imgs_name,key_pts,path):\n",
        "  imgs = []\n",
        "  orignal_imgs = []\n",
        "  i = 0\n",
        "  for image_name_ in imgs_name:\n",
        "    img = load_img(path+image_name_)\n",
        "    #rescale\n",
        "    w,h = img.size\n",
        "    key_pts[i][:,0] = (224/w) * key_pts[i][:,0] #224 is new Size \n",
        "    key_pts[i][:,1] = (224/h) * key_pts[i][:,1]\n",
        "    img = img.resize((224,224))\n",
        "    orignal_imgs.append(np.array(img)) #imamge on coloerd system\n",
        "    img = img.convert('L')\n",
        "    img = np.array(img)\n",
        "    imgs.append(img)\n",
        "    i = i+1\n",
        "  imgs = np.array(imgs)\n",
        "  orignal_imgs = np.array(orignal_imgs)\n",
        "  return imgs,key_pts,orignal_imgs\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnKByVIBKYWF"
      },
      "source": [
        "\n",
        "x_test,y_test,test_imgs =  load_data(test_imgs_name,test_key_pts,'/content/drive/MyDrive/P1_Facial_Keypoints-master/data/test/')\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvRTJYxJRCos"
      },
      "source": [
        "x_train,y_train,train_imgs = load_data(train_imgs_name,train_key_pts,'/content/drive/MyDrive/P1_Facial_Keypoints-master/data/training/')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRW7smkyaTnv"
      },
      "source": [
        "Normalize keypoints to be between 0&1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "butjtwLsYJ2-"
      },
      "source": [
        "x_test = x_test.reshape(770,224,224,1)/255.0\n",
        "x_train = x_train.reshape(3462,224,224,1)/255.0"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4jduSTbaYaW"
      },
      "source": [
        "Normalize keyPoints\n",
        "mean = 100, sqrt = 50, so, pts should be (pts - 100)/50\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epLjogBAYVnV"
      },
      "source": [
        "y_test = (y_test -100)/50.0\n",
        "y_train = (y_train -100)/50.0"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_LzG6D-a-82"
      },
      "source": [
        "CNN Architecture\n",
        "A common architecture for a CNN is a stack of Conv2D and MaxPooling2D layers followed by a few denesly connected layers. To idea is that the stack of convolutional and maxPooling layers extract the features from the image. Then these features are flattened and fed to densly connected layers that determine the class of an image based on the presence of features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81N6XyRnYaO9"
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "#Adding Dense Layers\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(136))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sajIdeOtYeT2",
        "outputId": "abec148d-0e35-4cc8-a7c5-280f006d0e46"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 222, 222, 32)      320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 111, 111, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 109, 109, 32)      9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 54, 54, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 52, 52, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 26, 26, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 24, 24, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 10, 10, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3200)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 136)               435336    \n",
            "=================================================================\n",
            "Total params: 574,184\n",
            "Trainable params: 574,184\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjGuCT1IbKa_"
      },
      "source": [
        "**Training**\n",
        "Now we will train and compile the model using the recommended hyper paramaters from tensorflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPwdE14bYrIe",
        "outputId": "f0fa544b-5eb4-4efc-d89f-b02befaca489"
      },
      "source": [
        "model.compile(optimizer = 'adam',loss = 'mean_squared_error')\n",
        "\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "'/content/drive/MyDrive/CSv/face_model', monitor='val_acc', verbose=1, save_best_only=False, mode='max')\n",
        "\n",
        "history = model.fit(x_train,y_train.reshape(3462,136), epochs=20,batch_size=10,callbacks=[model_checkpoint_callback], \n",
        "                    validation_data=(x_test, y_test.reshape(770,136)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "347/347 [==============================] - 202s 580ms/step - loss: 0.0870 - val_loss: 0.0542\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/MyDrive/CSv/face_model\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CSv/face_model/assets\n",
            "Epoch 2/20\n",
            "347/347 [==============================] - 200s 577ms/step - loss: 0.0600 - val_loss: 0.0480\n",
            "\n",
            "Epoch 00002: saving model to /content/drive/MyDrive/CSv/face_model\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CSv/face_model/assets\n",
            "Epoch 3/20\n",
            "347/347 [==============================] - 200s 575ms/step - loss: 0.0537 - val_loss: 0.0328\n",
            "\n",
            "Epoch 00003: saving model to /content/drive/MyDrive/CSv/face_model\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CSv/face_model/assets\n",
            "Epoch 4/20\n",
            "347/347 [==============================] - 200s 577ms/step - loss: 0.0316 - val_loss: 0.0199\n",
            "\n",
            "Epoch 00004: saving model to /content/drive/MyDrive/CSv/face_model\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CSv/face_model/assets\n",
            "Epoch 5/20\n",
            "347/347 [==============================] - 200s 577ms/step - loss: 0.0211 - val_loss: 0.0153\n",
            "\n",
            "Epoch 00005: saving model to /content/drive/MyDrive/CSv/face_model\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CSv/face_model/assets\n",
            "Epoch 6/20\n",
            "347/347 [==============================] - 201s 580ms/step - loss: 0.0151 - val_loss: 0.0125\n",
            "\n",
            "Epoch 00006: saving model to /content/drive/MyDrive/CSv/face_model\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CSv/face_model/assets\n",
            "Epoch 7/20\n",
            "347/347 [==============================] - 200s 577ms/step - loss: 0.0136 - val_loss: 0.0113\n",
            "\n",
            "Epoch 00007: saving model to /content/drive/MyDrive/CSv/face_model\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CSv/face_model/assets\n",
            "Epoch 8/20\n",
            "347/347 [==============================] - 200s 575ms/step - loss: 0.0110 - val_loss: 0.0097\n",
            "\n",
            "Epoch 00008: saving model to /content/drive/MyDrive/CSv/face_model\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CSv/face_model/assets\n",
            "Epoch 9/20\n",
            "347/347 [==============================] - 200s 576ms/step - loss: 0.0091 - val_loss: 0.0115\n",
            "\n",
            "Epoch 00009: saving model to /content/drive/MyDrive/CSv/face_model\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CSv/face_model/assets\n",
            "Epoch 10/20\n",
            "347/347 [==============================] - 202s 582ms/step - loss: 0.0078 - val_loss: 0.0085\n",
            "\n",
            "Epoch 00010: saving model to /content/drive/MyDrive/CSv/face_model\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CSv/face_model/assets\n",
            "Epoch 11/20\n",
            "347/347 [==============================] - 201s 580ms/step - loss: 0.0064 - val_loss: 0.0075\n",
            "\n",
            "Epoch 00011: saving model to /content/drive/MyDrive/CSv/face_model\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CSv/face_model/assets\n",
            "Epoch 12/20\n",
            "347/347 [==============================] - 200s 578ms/step - loss: 0.0053 - val_loss: 0.0077\n",
            "\n",
            "Epoch 00012: saving model to /content/drive/MyDrive/CSv/face_model\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CSv/face_model/assets\n",
            "Epoch 13/20\n",
            "347/347 [==============================] - 201s 578ms/step - loss: 0.0045 - val_loss: 0.0073\n",
            "\n",
            "Epoch 00013: saving model to /content/drive/MyDrive/CSv/face_model\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CSv/face_model/assets\n",
            "Epoch 14/20\n",
            "347/347 [==============================] - 200s 576ms/step - loss: 0.0046 - val_loss: 0.0070\n",
            "\n",
            "Epoch 00014: saving model to /content/drive/MyDrive/CSv/face_model\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CSv/face_model/assets\n",
            "Epoch 15/20\n",
            "347/347 [==============================] - 200s 575ms/step - loss: 0.0039 - val_loss: 0.0069\n",
            "\n",
            "Epoch 00015: saving model to /content/drive/MyDrive/CSv/face_model\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CSv/face_model/assets\n",
            "Epoch 16/20\n",
            "347/347 [==============================] - 200s 577ms/step - loss: 0.0034 - val_loss: 0.0064\n",
            "\n",
            "Epoch 00016: saving model to /content/drive/MyDrive/CSv/face_model\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CSv/face_model/assets\n",
            "Epoch 17/20\n",
            "347/347 [==============================] - 200s 576ms/step - loss: 0.0030 - val_loss: 0.0071\n",
            "\n",
            "Epoch 00017: saving model to /content/drive/MyDrive/CSv/face_model\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CSv/face_model/assets\n",
            "Epoch 18/20\n",
            "347/347 [==============================] - 200s 576ms/step - loss: 0.0029 - val_loss: 0.0066\n",
            "\n",
            "Epoch 00018: saving model to /content/drive/MyDrive/CSv/face_model\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CSv/face_model/assets\n",
            "Epoch 19/20\n",
            "347/347 [==============================] - ETA: 0s - loss: 0.0027"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meppLyBSb5dv"
      },
      "source": [
        "predictions = model.predict(x_test.reshape(770,224,224,1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O05nXhwcAoQ"
      },
      "source": [
        "p = (predictions*50)+100 #unnormalize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xcqefzWcDkX"
      },
      "source": [
        "for i in range(20):\n",
        "  show_keypoints(test_imgs[i].reshape(224,224),p[i].reshape(68,2))\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}